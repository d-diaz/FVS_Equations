{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Asynchronous parallel processing of FVS keyfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "import ipyparallel as ipp\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to execute FVS that will be mapped to all keyfiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fvs(keyfile):\n",
    "    fvs_exe = 'C:\\\\FVSbin\\\\'+os.path.split(keyfile)[-1][:5]+'.exe'\n",
    "    subprocess.call([fvs_exe, '--keywordfile='+keyfile]) # run fvs\n",
    "    \n",
    "    base_dir = os.path.split(keyfile)[0]\n",
    "    base_name = os.path.split(keyfile)[-1].split('.')[0]\n",
    "    \n",
    "    # clean-up the outputs\n",
    "    # move the .out and .key file\n",
    "    path = os.path.join(base_dir, 'completed','keyfiles')\n",
    "    if not os.path.exists(path): \n",
    "        os.makedirs(path)\n",
    "    shutil.move(keyfile, os.path.join(base_dir,'completed','keyfiles'))\n",
    "    path = os.path.join(base_dir, 'completed','outfiles')\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    shutil.move(os.path.join(base_dir,base_name+'.out'), os.path.join(base_dir,'completed','outfiles'))\n",
    "    \n",
    "    # delete the other files\n",
    "    # os.remove(os.path.join(base_dir, base_name+'.trl'))\n",
    "    return keyfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following command in a command prompt to start up a cluster of workers:\n",
    "\n",
    "`>> activate Py3.5 # or other environment name`\n",
    "\n",
    "`(Py3.5)>> ipcluster start -n 4 # or other number of cores`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a hub to control the workers\n",
    "c = ipp.Client()\n",
    "c.ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a direct view of the workers and a load-balanced view for submitting jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing subprocess on engine(s)\n",
      "importing shutil on engine(s)\n",
      "importing os on engine(s)\n"
     ]
    }
   ],
   "source": [
    "dv = c[:] # direct view\n",
    "v = c.load_balanced_view() # load-balanced view\n",
    "\n",
    "# import packages to all workers\n",
    "with dv.sync_imports():\n",
    "    import subprocess\n",
    "    import shutil\n",
    "    import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute an ayschronous batch of FVS runs for all the keyfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8,273 keyfiles found.\n"
     ]
    }
   ],
   "source": [
    "# gather the list of keyfiles to run\n",
    "run_dir = os.path.abspath('keyfiles_to_run')\n",
    "to_run = glob.glob(os.path.join(run_dir, '*.key'))\n",
    "print('{:,}'.format(len(to_run)), 'keyfiles found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started batch processing.\n"
     ]
    }
   ],
   "source": [
    "# start asynchronous batch with load-balanced view\n",
    "res = v.map_async(run_fvs, to_run)\n",
    "print('Started batch processing.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monitor progress of batch run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "531ca5b6202643eead7109fc3ca73160"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Default method\n",
    "# res.wait_interactive()\n",
    "\n",
    "# OR USE A PROGRESS BAR!\n",
    "from tqdm import tqdm_notebook\n",
    "import time\n",
    "\n",
    "runs_done = res.progress\n",
    "with tqdm_notebook(total=len(res), initial=runs_done, desc='FVS Run Progress', unit='keyfile') as pbar:\n",
    "    while not res.ready():\n",
    "        new_progress = res.progress - runs_done\n",
    "        runs_done += new_progress\n",
    "        pbar.update(new_progress)\n",
    "        time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Human time spent:', res.wall_time)\n",
    "print('Computer time spent:', res.serial_time)\n",
    "print('Async speedup:', res.serial_time/res.wall_time)\n",
    "print('Human time per FVS run:', res.wall_time/res.progress)\n",
    "print('Computer time per FVS run:', res.serial_time/res.progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.abort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a true/false if full set of jobs completed\n",
    "# res.ready()\n",
    "\n",
    "# Cancels the batch (wait for fvs executables to complete)\n",
    "# res.abort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect how processing speed per run changed as batch progressed\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "time_steps = \n",
    "#[(t2 - t1).total_seconds() for t2, t1 in zip(res.received, res.submitted)]\n",
    "plt.plot(time_steps)\n",
    "plt.ylabel('time per run')\n",
    "plt.xlabel('runs completed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(glob.glob('C:/GitHub/FSC_Case_Studies/keyfiles_to_run/completed/keyfiles/*.key'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shut down the parallel workers\n",
    "c.shutdown(hub=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for runs that didn't complete successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "conn = psycopg2.connect(\"dbname='FVSOut' user='postgres' host='localhost'\") # password in pgpass file\n",
    "SQL = '''\n",
    "SELECT keywordfile, caseid, mgmtid\n",
    "FROM cases;\n",
    "'''\n",
    "# read the query into a pandas dataframe\n",
    "completed = pd.read_sql(SQL, conn)\n",
    "\n",
    "# close the database connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pd.unique(completed.caseid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(completed.caseid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed['keyfile'] = completed.keywordfile.apply(lambda x: os.path.split(x)[-1] + '.key')\n",
    "completed.keyfile.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counts = \n",
    "#counts.loc[counts.keyfile > 1]\n",
    "cases_count = completed.groupby('caseid').count()\n",
    "dupes = cases_count.loc[cases_count.keyfile > 1]\n",
    "dupes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_keys = glob.glob('C:\\\\GitHub\\\\FSC_Case_Studies\\\\keyfiles_to_run\\\\completed\\\\keyfiles\\\\*.key')\n",
    "completed_basenames = [os.path.split(x)[-1] for x in completed_keys]\n",
    "print(len(completed), 'keyfiles in database')\n",
    "print(len(completed_basenames), 'keyfiles in completed folder')\n",
    "\n",
    "for keyfile in completed.keyfile.values: # keyfiles recorded in the DB\n",
    "    if keyfile not in completed_basenames:\n",
    "        print(keyfile)\n",
    "\n",
    "for keyfile in completed_basenames: # keyfiles recorded in the DB\n",
    "    if keyfile not in completed.keyfile.values: # keyfiles moved into output folder\n",
    "        print(keyfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_rerun = ['fvsPN_stand11299_rx3_off0.key', 'fvsPN_stand1862_rx2inner_off0.key', 'fvsPN_stand2369_rx4_off0.key', \n",
    "#             'fvsPN_stand2369_rx5_off0.key', 'fvsPN_stand237_rx5_off0.key', 'fvsPN_stand239_rx3_off0.key', \n",
    "#             'fvsPN_stand319_rx5inner_off0.key', 'fvsPN_stand4225_rx1_off0.key', 'fvsPN_stand5802_rx5inner_off0.key',\n",
    "#             'fvsPN_stand7307_rx3_off0.key']\n",
    "\n",
    "# for keyfile in to_rerun:\n",
    "# # move the keyfile back into the keyfiles_to_run directory\n",
    "#     outfile = keyfile.split('.key')[0] + '.out'\n",
    "#     base_dir = 'C:/GitHub/FSC_Case_Studies/keyfiles_to_run'\n",
    "#     #shutil.move(os.path.join(base_dir,'completed','keyfiles', keyfile), os.path.join(base_dir,keyfile))\n",
    "#     shutil.move(os.path.join(base_dir,'completed','outfiles', outfile), os.path.join(base_dir,outfile))\n",
    "#     print('Moved', outfile, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = completed.groupby('keywordfile').count()\n",
    "counts.loc[counts.keyfile > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\"dbname='FVSOut' user='postgres' host='localhost'\") # password in pgpass file\n",
    "SQL = '''\n",
    "SELECT mgmtid, cases.caseid, cases.standid, keywordfile, total_stand_carbon, rbdft, summary.year\n",
    "FROM cases, summary, carbon\n",
    "WHERE cases.caseid = summary.caseid AND cases.caseid = carbon.caseid AND summary.year = carbon.year AND cases.standid = summary.standid AND cases.standid = carbon.standid;\n",
    "'''\n",
    "# read the query into a pandas dataframe\n",
    "completed_attributes = pd.read_sql(SQL, conn)\n",
    "\n",
    "# close the database connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_attributes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_atts = completed_attributes.groupby(['mgmtid', 'standid']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_attributes.loc[completed_attributes.year == 2014][['mgmtid', 'standid']].groupby(['mgmtid']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_atts.loc[count_atts.rbdft != 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# failed = glob.glob('C:\\\\GitHub\\\\FSC_Case_Studies\\\\keyfiles_to_run\\\\PN\\\\completed\\\\outfiles\\\\failed\\\\*.out')\n",
    "# failed_basenames = [os.path.split(x)[-1].split('.')[0] for x in failed]\n",
    "# moved = glob.glob('C:\\\\GitHub\\\\FSC_Case_Studies\\\\keyfiles_to_run\\\\*.key')\n",
    "# moved_basenames = [os.path.split(x)[-1].split('.')[0] for x in moved]\n",
    "# for path in moved:\n",
    "#     if os.path.split(path)[-1].split('.')[0] not in failed_basenames:\n",
    "#         print(path, \"not in failed, but was moved\")\n",
    "# for path in failed:\n",
    "#     if os.path.split(path)[-1].split('.')[0] not in moved_basenames:\n",
    "#         print(path, \"not in moved, but failed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Py3.5]",
   "language": "python",
   "name": "conda-env-Py3.5-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
